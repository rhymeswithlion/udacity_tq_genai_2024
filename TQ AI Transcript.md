# AI: Executive Briefing

## Introduction

Artificial intelligence can get started with generative AI. Artificial intelligence or AI. Generative AI. Something's wrong. Let's talk about artificial intelligence. No, no, no, no, no, no, no. That was weird. Welcome. Artificial intelligence is a subject I've been teaching for years. But in the past, I often encountered this roadblock where people were willing to recognize AI was important but, you know, somewhere else or for someone else. They could agree it was important for the company or important for that big external project. It just didn't seem relevant to them as individuals. People would often tell me they didn't expect to personally use AI in any significant way in their own day‑to‑day work and life. But that belief changed at the end of 2022\. ChatGPT reaches 100 million users in 2 months. AI could automate 300 million jobs. It will change the way we think can work. It's coming for our jobs. Here's the 10 roles AI is likely to replace. ChatGPT is banned in Italy over privacy concerns. It's a malevolent AI and should be destroyed. ChatGPT is about to revolutionize the economy. We have never seen this level of interest in AI or this level of anxiety, incredible enthusiasm mixed with equal amounts of apprehension. Where's this all going? What's going to happen next? Well that's what we're going to cover. This is a short introduction to artificial intelligence or AI. And we'll talk about these latest AI‑driven tools like ChatGPT, Google Bard, DALL‑E, and others, tools we can use to generate new content, whether that's text or images, audio, video, computer code. I'm going to show you what these tools are good at, what they're not good at, and why they're not good at certain things. But to really understand these most recent tools, we do need a little history because they didn't appear out of nowhere. They're the latest in a long evolution of artificial intelligence that goes back decades. So we'll quickly go over that timeline and get the background we need. Along the way, we'll explore the most important ideas about AI and a lot of the vocabulary, terms like machine learning, deep learning, neural networks, generative AI. We'll talk about large language models and foundation models and more and what all of it means. But this isn't just about learning a few definitions. It's about recognizing where AI can fit and where it can make a difference. So we're not just going to talk about the ways it might impact your company or the projects you're working on but how it's going to impact you personally because it will. It is already, directly or indirectly, and it will continue to do so more and more. And I get it, you know this, so let's dive in. And we'll begin with a very common question, but it's one of those questions a lot of people don't want to ask. What exactly is artificial intelligence? What does this term even mean? Is there a basic definition we could learn and from that point on easily say, okay, that is artificial intelligence, but that is not. And as strange as it might sound, the answer is no. There is no simple agreed‑upon definition that will always let us make that distinction. However, the question, what is artificial intelligence, is still an excellent question, and it's a great place to begin.

## Defining Artificial Intelligence

Artificial intelligence is often talked about as this ultra‑modern, cutting‑edge, futuristic area of technology. And okay, it can be, but it does have a long history. I have a few textbooks here about artificial intelligence. This one is 30 years old. This one's 40 years old. This one's nearly 60 years old because this has been a serious field of study since the 1950s. But even back then, they admitted this term, artificial intelligence, is kind of tricky. We might want a simple straightforward definition. We're not going to get one. For example, in this paper on artificial intelligence, which was written in 1958 by Marvin Minsky who was one of the major researchers in the field. He co‑founded the AI Lab at MIT. And I'll summarize just two sentences of this, that it isn't useful to lay down an absolute definition for intelligence or intelligent behavior for our goals in trying to design thinking machines are constantly changing. You see the problem with this term, artificial intelligence, isn't that the words are difficult. We know what artificial means; we know what intelligence means. The problem is the goalposts keep moving. You see, across the entire history of computing, we've always had new goals, new aspirations, problems we wanted to solve, but we didn't know how. And some of these goals were considered so difficult, they really didn't seem possible without genuine intelligence on the part of the computer. These goals included things like will a computer ever play chess at the level of a grand master? Can a computer ever recognize faces or understand spoken commands? Now sure, we know how to do all of those things now. But what always happened is as soon as anyone figures out how to accomplish a goal, the response then becomes, well, that's great. But that's not really intelligence, is it? The computer isn't actually thinking. It's just executing a formula. It's just a computation. There's even a name for this. We call it the AI Effect. And a lot of tech we now use every day began as some kind of artificial intelligence research. But as soon as it becomes actually implemented, practical, and out there in the world, nobody wants to call it AI anymore. There are tasks you can do on your phone that a few years ago would have been considered remarkable, incredible implementations of artificial intelligence. As just one example, we have facial recognition that's so fast I can turn myself into a 3D cartoon in real time. Now if you'd shown this at an artificial intelligence conference 10 years ago, you would have been the star of the show. These days, most people wouldn't call this AI. It's just a cute thing you can do on your phone if you even care. So we're not going to worry about getting to this perfect definition for AI. However, we do need a way to at least broadly categorize what are the kinds of things we typically call artificial intelligence as opposed to regular conventional computer programming? But we have to be careful. Because AI has been around so long, not just as an area of research and development, but also in movies and TV and books and games, we all have ideas and preconceptions about what artificial intelligence is, what it's capable of, or what we think it should be capable of. You can't help it. It's just part of the culture. There are cliches about AI that have become so pervasive that everybody just becomes numb to them. I'm going to show you a few of those cliches because some of them are useful and some of them get in the way.

## AI Clichés, Good and Bad

Artificial intelligence as a subject has a lot of common cliches and preconceptions, the things that everybody kind of knows about this. There's even visual cliches. For example, have you noticed that in our current culture, artificial intelligence has a color? Apparently, we all think AI is blue. Now if you're thinking, Simon, what are you talking about? Let me prove it. I'm about to do an image search for the term artificial intelligence. We'll gather pictures from all over the internet that have been given this title or associated with this idea. There are thousands of images created by countless different artists and designers, but they are strangely consistent. It's as if there's only two ways you're allowed to show AI. You either need a wireframe image of a brain made of nodes, little dots with interconnecting lines. Your other option is a robot, but not any robot. It needs to have a white plastic face and, for bonus points, exposed head wiring. So let's see. I go to images.google.com, and I search on AI, and we get node brain, node brain, white‑faced robot, white‑faced robot with exposed head. It just keeps going. And notice, almost everything is blue, and this isn't just true for images on the web. You'll find the same thing with book covers and articles and presentations. It's like all the graphic designers had a secret meeting and agreed, well, AI is blue obviously. And okay, I'm poking fun at this, but there is a point. When you begin to notice these cliches and the next time you see a book or an article about AI and right there at the top is a big blue image of a robot with a creepy doll face, you think, okay, I've seen this cliche before. What other AI cliches am I about to get? Now some cliches can actually be useful. They give us starting points and ideas to talk about, but I find these kinds of images everywhere just isn't helpful. It's distracting because they don't represent what we're typically trying to do. I mean, I don't know what you're going to do next with AI. But if I had to guess two things you're not trying to do, I don't think you're trying to model a human brain, and I don't think you're trying to make a robot. And okay, both of these ideas, brain modeling and robotics, are genuine, active areas of AI research. But it's not what most businesses are focused on It's not what most businesses or individuals care about. So you won't be getting any more node brain or white‑faced robot images from me. But I did say there's also useful cliches. So let's touch on perhaps the most common one you find in movies, TV, and novels, and that's the idea of an AI that operates at the level of a person.

## AI and AGI

When artificial intelligence is featured in movies, TV, games, books, it's usually a character of some kind. Sometimes that AI character has a body, whether humanoid or a machine. Other times, the AI is running on some massive computer behind the scenes. But it's typically shown as being capable of complex, multilayered conversations, similar to and possibly higher than the level of a human. And where it isn't just capable of one or two specific narrow abilities, but it's generally intelligent. These fictional AIs can reason, they have multiple skills, and they're capable of learning new things by themselves. And this idea takes us beyond the term AI and into what's called artificial general intelligence, or AGI, sometimes called strong AI or full AI. Now let me be clear. AGI is still hypothetical. It's theoretical. It only exists in fiction. We don't have AGI yet. We may never have it. What we do have available is a kind of AI sometimes called narrow AI. Now this is often extremely powerful and can outperform a person, but it typically works in one specific narrow ability. For example, we have AI right now that could play chess better than anyone in the world. We have other AI that's wonderful at identifying objects and images. We have AI that can scour massive amounts of data and provide insights about that data. But just because one of these AI programs might be superb at detecting a spam email or be really good at recognizing an abnormal heart rhythm, it doesn't mean that same program can also play chess and compose a sonnet and recommend the next TV show to watch. And that's okay. For the most part, what we want is specialized intelligence, narrow intelligence, because if we're trying to use AI to accomplish or understand something in particular, it's okay if that program is task‑specific. Now the latest generation of AI tools, like ChatGPT and Google Bard, one of the reasons they've made such an impact so quickly is they are more generally applicable than anything we had before where you can use them to accomplish a wider and more flexible set of tasks. For example, you might use ChatGPT to help you write a resume. But it isn't just an AI that writes resumes. It's not that narrow, it's capable of much more. You might then paste in some complex computer code and ask ChatGPT to explain it to you. You could then ask it to create a packing list for your trip to Alaska and then to summarize a business article and then write a poem about quantum mechanics. It's very powerful. It's very flexible. But it is still narrow AI in that what it's capable of, generating text content in response to direct prompts. ChatGPT isn't capable of reasoning or independent decision‑making. So despite some clickbait headline articles you might see about these new tools and whether we've reached AGI yet, the short answer, no we haven't. You might describe these tools as a step in that direction because they are more generally applicable, but we're still a long way from AGI. Now a little later, I'll show you some specific examples of how easy it is to reach the limitations of what these tools are currently capable of. Now if you're wondering, okay, so is anybody trying to develop artificial general intelligence? Oh yes. There's billions being invested in it. For example, OpenAI, the company who make ChatGPT, their goal, their mission as a company, is artificial general intelligence. You find the term AGI everywhere in their own descriptions of what they do and what they're trying to do. Another example, DeepMind owned by Google is very clear that their long‑term aim is also to develop artificial general intelligence, or AGI. So yes, there's a lot of activity, effort, and investment around this. But even if we don't have AGI right now, we do still have some very impressive AI. So let's talk about one of the ways AI is being implemented today, and that's using machine learning.

## AI and Machine Learning

Over the past few years, the most common, most successful, most practical approach to implement AI has been with machine learning, and it's led to some confusion about those terms. Machine learning and artificial intelligence are mentioned together so often, it can sound like they mean the same thing. They don't. Artificial intelligence is a broad and, okay, sometimes vague term that we could basically consider all the various different approaches and methods that people have used to make a computer program do something smart. And machine learning is a set of specific techniques to do that. But machine learning is not the only way to implement AI. Machine learning, or ML, is a type of AI. It's a subset of it. All machine learning is artificial intelligence, but the broader field of artificial intelligence does include other techniques than just machine learning. But if you want to understand what machine learning does, let's first consider a normal computer program that doesn't use ML or any kind of AI. You see, in regular conventional computer programming, which includes most applications on your desktop or your laptop or your phone, a software developer tries to solve a problem by providing a bunch of rules and very specific instructions. If this number is greater than that number, then add these two things together. If this date is less than that other date, generate this error message. If the position of the asteroid on the screen is now the same as the position of the spaceship, then play boom sound effect and display Game Over. Now make no mistake. Conventional, rule‑driven computer programming is fantastic when you know what all the rules are and you can cleanly define them. But with machine learning, you don't begin by trying to figure out and define all the rules in advance. Instead, you begin by gathering examples to learn from. Here's what I mean. Let's say I wanted a computer program that could take a small, digitized image and be able to recognize handwritten numbers, handwritten digits, in that image. Well let's make it even simpler. Let's say I want to write a program that could recognize the number 3\. Now if I take the conventional programming approach, I might try writing code that would take each image and look at every individual pixel one by one, figure out if it's light or dark, and then try and envision and describe all the different ways people might ever write a number 3 in this grid. Sometimes it's straight. Sometimes it leans to the left or to the right. Sometimes the top loop is smaller than the bottom loop, etc., etc, and that would be the conventional programming approach. Figure out all the rules, go step by step, try and envision every possible eventuality. And to be clear, it would be very hard, tedious work and with this particular problem, extremely easy to get it wrong. Now, if we instead take the machine learning approach, we don't begin by trying to figure out all those rules in advance. Instead, we begin with actual data, real data. In this case, the data would be thousands of different images of a handwritten number 3, and we use that data to build our program. And before doing anything first, we need to clean and prepare the data. Make sure there's nothing extra, nothing missing. If I'm trying to teach a computer to recognize a number 3, then I don't want this data to also include images with a 2 or a 4 or some other character. I don't want blank images. Now once the data is cleaned and prepared, we take all of it and feed it into a machine learning algorithm in a process which is called training a model. And after we've trained the model with all that data, we could then expect to be able to take a new piece of input, a new image, pass it over to that newly trained model and have it evaluate, does this new image look like a 3 or not? We've provided enough examples that it could recognize the characteristics of that thing. And this is one of the tasks machine learning is very, very good at, classification. And it doesn't just apply to images. Classification is used with all kinds of data because it's a very common problem in computing, to get some new data and be able to ask what is this. When we've got incoming emails, could we quickly classify them as these ones look like spam and these ones look legitimate? Machine learning's ability to categorize or classify helps in all sorts of situations. It's used for diagnosing medical conditions, fraud detection, evaluating social media comments. But classification isn't the only area where machine learning can help. There's a few other kinds of problems it's very good at, including regression, where we'd still begin with data to analyze and learn from. But instead of using that data to classify or categorize things, with regression, we use the data to predict a number of some kind, like an amount, a price, or a time, some kind of numerical value based on analyzing all the other data that might affect that value. Our classic example of regression is using it to predict a house price based on multiple other factors like square footage, number of bedrooms, location, age of the house, etc. But in agriculture, you might use regression to predict crop yields based on multiple other variables, like the type of crop, the current weather conditions, the soil quality. If we had prior data about medical procedures and patient characteristics, we could then use regression to make better predictions that for a new patient having this procedure at this age with these comorbidities, what is the likely length of their hospital stay? Regression and classification are two of the most common applications of machine learning. There are other things we can do with it, like clustering and anomaly detection. But that's outside the scope of this course. If you're interested in knowing more about machine learning, I do have a course just on that that goes a little deeper. But there is a catch. Machine learning can only work if you have data to learn from and not just a little bit. You need a lot of data and good quality data. This is why there's been such a focus on data collection and big data in the business world over the last few years because if you haven't gathered enough data, you will have nothing for your machine learning algorithms to learn from.

## Machine Learning Frameworks

In any machine learning situation, there's two essential pieces. First, you need data, which you have to provide. But you also need this machine learning algorithm that's able to learn from that data. And often people wonder, well, that sounds like the difficult part. So, did the software developers in my organization have to write this machine learning algorithm? And no, most organizations don't write and don't need to write machine learning algorithms. You can just find one that's already been written and feed your data into it. It's the same way that if you want a database, you would choose an existing database management system software like Oracle or SQL Server, even Microsoft Access. If you want to make your website, you would choose a web framework or a hosted service. So here, when you're using machine learning, you typically use an existing machine learning platform or machine learning framework. There's lots of them, both commercial and open source. There are cloud‑based machine learning features available in Microsoft Azure or Google Cloud or Amazon's AWS or with IBM. And with any of these, you can then pick and choose from a bunch of different machine learning algorithms depending on the data you have and what you're trying to do. And this is what's made applied artificial intelligence just blow up in the last few years. It's become so easy to get started when you don't have to build all this background infrastructure from scratch. But even though you don't have to write the algorithms yourself, you have to do some work. You can't just throw all of your corporate data at a machine and expect it to magically make sense of everything for you. Think about it this way. Let's say somebody handed me a flash drive with a year's worth of extremely detailed website activity data, information about every single visit, what that person did, what pages they visited, what terms they searched on, how long they spent, what they clicked. Did they purchase anything? Did the order actually go through? Now, this data almost certainly contains information about fraudulent purchases that we might be able to use to help recognize and prevent future fraud. But this same data also contains information about high‑value customers in a way that we could recognize new ones. And it probably also contains useful information about identifying hacking attempts. It's all in there somewhere. So first and foremost, we have to deeply understand what is it we're looking for right now? What are we trying to train the machine to learn? And the most important part of the entire machine learning process is preparing your data, understanding what it is you're looking for and then filtering your data, cleaning it, labeling it, throwing out the stuff that's not relevant for this particular problem. Because if your data is full of garbage with invalid values, missing values, conflicting information, it doesn't matter how good the machine learning algorithms are. As ever in programming, garbage in, garbage out. If your organization has a team of people working on machine learning projects, this is one of the responsibilities of the data scientist role of transforming the data into something meaningful and usable for machine learning algorithms. And after that, choosing the correct algorithm, applying it, configuring it, testing it, that's another part that takes time and attention and very importantly understanding what you're trying to accomplish. Most data scientists would tell you it isn't a single task. It's a process where they'll often experiment with trying several different machine learning algorithms and test them to figure out which algorithm gives the best results for that particular problem with that particular data. And machine learning has been one of the massive successes of AI with widespread adoption across a range of industries and applications. And even this latest generation of AI applications like ChatGPT, Google Bard, and DALL‑E, they are all types of machine learning. Yes, they're more advanced than just simple classification or regression, but they're built on the same foundations. So let's talk about some more advanced aspects of machine learning.

## Deep Learning and Neural Networks

Earlier, I talked about the visual cliche of the wireframe brain. I even dismissed it by saying, you know most of us using AI, we're not trying to model a brain. However, just as machine learning is a subset of AI, a type of AI, there's also a smaller subset of machine learning called deep learning, and it uses techniques that are more to do with the brain. These techniques are called neural networks. And okay, neural networks aren't really trying to model a human brain. They are inspired by a teeny little part of it, the idea that our brains have billions of neurons and synaptic connections, neurons that have multiple connections to other neurons. And in an artificial neural network, this idea is somewhat modeled in software. So there are simulated neurons, individual nodes that can be connected to and send messages to other simulated neurons. Now, in the past, I used to dive a little deeper into how these neural networks are constructed, and I'd show diagrams talking about the different layers. But to be honest, if your goal is to be a successful and effective end user of AI tools, then knowing the technical details of neural networks really doesn't matter that much. Instead, let's focus on this. What's it good at? Well, that's what's given us the best results in very complex problems like image processing, facial recognition, speech recognition, complex game playing. Deep learning is capable of very impressive, very profound results. But there are downsides because training a deep learning model is much more computationally intensive. It takes much longer than some of the other simpler machine learning methods. And part of that reason is deep learning also typically requires a lot more data to train properly. Now it's always difficult to say exactly how much data you need to train a machine learning model. It's very situation‑dependent. But it certainly wouldn't be unusual to expect to have tens of thousands, hundreds of thousands, even millions of examples in order to train a deep learning model well. And because of this, this can also be incredibly expensive. Some of the more advanced deep learning models, like the one that ChatGPT uses, are estimated to cost millions of dollars in computing power alone to just train the model. Nothing to do with staffing or development costs, just the computing resources to train the model. And we're going to see more and more of this because these deep learning algorithms do give better results with massive amounts of training data, but they also take immense amounts of computing time and resources to run. But once the model is trained, it can be extremely powerful. So deep learning is often used to understand language, which is what we're going to talk about next.

## Natural Language Processing (NLP)

Hey Vector. ‑Ready. What is natural language processing? ‑Definition of natural language processing. The branch of information science that deals with natural language information. ‑That's technically true, but it's one of those times where the dictionary definition really doesn't help very much. Natural language processing, or NLP, is the area of AI that focuses on recognizing, understanding, analyzing, even emulating how humans communicate using either speech or written text or both. One example of where NLP is used is with any of the various personal voice assistants. And I don't want to wake up any device you might have, but voice assistants like or or or. When you talk to these devices, you're asking them to do much more than just recognize speech to text. This is not like dictation software where all it needs to do is recognize a stream of individual disconnected words. No. Here, the AI needs to parse and understand what it is we're actually talking about and the variety of different ways we might phrase the same thought. The goal is to successfully identify the meaning, the semantic content of our sentence, even if it wasn't well phrased or even a well‑formed sentence at all. Was it a command? Was it a question? What was the subject? What are the keywords? And this can be a lot trickier than it sounds. For example, I might ask any of these voices assistants, will I need an umbrella today? That's a simple natural language question. But most of the meaning of it is not being directly stated. It's all under the surface. This question isn't about an umbrella. This question is about the weather. It could be rephrased as get the weather forecast for my current location, ignore everything except the likelihood of rain, then make a judgment about whether that likelihood of rain is significant enough to matter. But I didn't mention my location. I didn't say weather forecast. I didn't say chance of precipitation. I just implied all those things, and I expected the computer to understand the question behind my question and actually answer that. And not only to understand it, but to generate a meaningful natural language response back to me. ‑It doesn't look like it's going to rain today. ‑But NLP isn't just for speech. It applies to written text as well. It can be used to analyze and summarize books and articles and web pages. It's used in language translation. It's used for chatbots and virtual assistants and often has multiple uses at the same time. Let's say you're having a video conference call. Well, this area of AI could be used to instantly transcribe what's being said and provide subtitles for it. Also translate that into other languages on the fly. And when the meeting is finished, write up a natural language plain English summary of what happened and send it to everybody who wasn't there. And as with any area of AI, with NLP, we can have both specific, narrower applications of this and more general, wider ones. An example of a more specific usage is sentiment analysis. This is commonly used in situations where you deal with a lot of online comments or reviews or social media posts. Sentiment analysis can scan that text and quickly identify whether those comments are positive or neutral or negative, even make judgments on emotion to identify happy or angry customers and hopefully deal with any major issues before they get out of hand. And for a more general wider application of natural language processing for systems that can do an incredible job of both understanding complex requests from us and generating complex answers back to us, let's dive into some of the text‑based generative AI applications that we now have.

## Introducing Generative AI

I've talked about several different categories of AI, but the one that's really gotten the world's attention and has had a monumental impact in a remarkably short amount of time is generative AI. Now this, as the name might suggest, is AI that is designed to generate content. And not from some prescripted, predefined set of options, but to create entirely new content. Some of these AIs create written text; others can generate images or computer code, even video or music. And they work by first being trained on massive, massive amounts of data, whether that's text or images or code. So a text‑based generative AI will have analyzed millions, even billions of text sources, web pages, books, articles, scientific papers. An image‑based generative AI will have been trained on countless images, photographs, illustrations, diagrams. And the combination of all the data they've been trained on, together with your suggestions, your prompts, allows them to then generate that brand new content. The most well known of these is ChatGPT from the company OpenAI. ChatGPT released in November 2022\. They hit a million users within 5 days, 100 million users within 2 months. Just incredible uptake in adoption and a lot of other tech companies suddenly scrambling to play catch‑up. In 2023, Google released their generative AI called Bard, and Microsoft also released a version of their Bing search engine that added generative AI features into the search experience. Now behind the scenes, the new Microsoft Bing is actually using a version of the same AI that ChatGPT uses. But the implementation of it is different, and you don't get identical results. Now the three that I've just mentioned are all text‑based. They all use an AI technique called a large language model, or LLM. And they use the deep learning and neural networks we talked about earlier. They're trained on staggering amounts of text content. They're very time‑consuming and very expensive to train. But once they're trained, large language models can generate text that is genuinely difficult to distinguish from text written by humans. Having said that, not all generative AI uses large language models because we have other options that are image‑based or audio or video. The better‑known ones here include DALL‑E, which is also from OpenAI, and it generates images based on prompts. We also have options like Midjourney and Stable Diffusion. Now with these, we don't use the term large language model. You may see the term foundation model used to describe them, which is a slightly more generic term. So all the large language models can be considered a type of foundation model, but foundation models describe other modalities like images and video as well. Now, the text‑based generative AI and the image‑based generative AI might seem like entirely different technologies, but they are fundamentally very similar. And I believe the image generation tools can actually be more useful and helpful to understand why and where generative AI gets things wrong because with all of them, whether they generate text or generate images or audio, they can be extremely good at it. They can give you incredible results, and they can be completely wrong all at the same time and for the same reason. Let's talk about why.

## Prompts and Prompt Engineering

Getting great results from generative AI often comes down to the quality and creativity of the prompts, the questions you're asking or the commands you're giving it. Now there's a term, sometimes called prompt engineering. It can mean different things to different people. But we can just take it as the idea of applying some structure, some formality and thought to our interactions with generative AI in order to steer it towards the results we want. And when we don't get the results we want, how we then interact with the system and how we refine and rephrase those prompts. Just as a casual example, and I'll intentionally begin with a very boring business use case. Let's say I've been putting off writing an email. So I ask ChatGPT to write an email to congratulate my colleague on their 10‑year anniversary at the company. And it does it. It comes back with some bland generic copy. ‑It's hard to believe a decade has already passed since you joined us. But during that time‑‑‑ ‑Blah blah, significant contributions, blah blah, inspiration to us all. And it's coming up with this bland inoffensive copy because it's analyzed lots of business communication where this is the kind of writing most people do most of the time. It's not a bad guess. It's not what I wanted. I'd never write this. So within this same chat, I just say make it more informal. I don't have to say anything else because it's part of the same chat, and ChatGPT is keeping track of previous context. So it recognizes I'm still talking about the previous email. The subject line now becomes ‑Woohoo\! Congratulations on hitting 10 years with us\! It's now swung the pendulum the other way. It's far too enthusiastic for me. There's lots of exclamation marks, and I'd never say something like it's not every day you hit the big 1‑0. Okay, so let's get more specific and a little bit more creative. Rewrite it as if my colleague and I are good friends and we both have a dark sense of humor and make it shorter. Now it's more like it. The subject line begins, Congrats on a decade of suffering. And we start with Holy crap, can you believe it? We have phrases like Congratulations on reaching this dubious milestone. It even finishes with Here's to 10 more years until one of us finally snaps and runs out of the building screaming. Okay, this might not be exactly what I'd write, but it absolutely did what I asked. And it's this idea that with slight changes in the way you ask for things, you can get very different results. If you just engage with these tools in an uncreative and predictable fashion, you will get uncreative, predictable results. And some of the most useful ideas to incorporate in your prompts include personas where you can tell a large language model to act as if it's a type of person. That can be a job role act as a marketing copywriter, act as a physics professor, act as a financial advisor. It could be specific people or historical figures or fictional characters. Act as Ada Lovelace and explain the analytical engine. Write this like a script by Quentin Tarantino. Assume the role of Hermione Granger. But you can also be specific about the audience. Explain this as if I'm 10 years old. Write as if you're talking to a room full of software developers. You can tell it what the desired tone should be and not just formal versus casual, but being very specific like my earlier example, write as if we're good friends and both have a dark sense of humor. You can be specific about the length. Do you want a short email or a long email? Do you want five tweets? Do you want 500 words on something or 3 paragraphs or 2 sentences? The more specific and creative you can be with your prompts, the more creative and interesting the result. Now let's, well, let's get ChatGPT to write the closing sentence for this clip. Act as a 1980's movie trailer announcer and write two sentences to explain the next clip is about how generative AI works and when it doesn't. In a world where AI becomes more pervasive every day, the mysteries of generative AI continue to baffle and amaze. Join us for an unforgettable ride as we delve into the fascinating workings of this cutting‑edge technology and uncover its surprising limitations. That's pretty good.

## Hallucinations—And When Generative AI Goes Wrong

To get an idea of how a generative AI works and also how it can go wrong, let's just begin with the autocomplete you get whenever you write a text message. As I write it, I get these suggestions. And the more I write the suggestions change and refine. So if I get to the point where my message begins, it's going to be a beautiful, what's the next word? Well, you could analyze millions of text messages. You could look at previous usage of these words or phrases like this, and you would probably find the most likely next word here is day. It's going to be a beautiful day or perhaps night. But it's very important to recognize this messaging app doesn't actually understand anything. It doesn't know my mood, what's going through my mind. It is simply a statistical likelihood that after writing these words, the next word is probably day or night. But the fact it's the most likely option doesn't mean it's correct. I might have just walked into a room decorated for an upcoming wedding reception, and I was about to write it's going to be a beautiful wedding. Could have been 1000 other things, beautiful cake, beautiful house, beautiful painting. But we can understand why the autocomplete suggested beautiful day. It is the option with the highest probability. But it can still be wrong. Okay, you might think what does that have to do with generative AI? Well, because in a way, a lot of generative AI works by trying to be this tremendously complex autocomplete. Given the fact that you just wrote all these words, here is the statistically most likely next word or phrase or entire paragraph. And as with autocomplete, with generative AI, you can often get a suggestion where you kind of understand why you got it, and it's still completely wrong. However, what can be a real problem is when you use generative AI as any kind of research assistant for something where you're not an expert. It can give you the wrong result and you don't know it's wrong because the results you get from generative AI are often so impressive, it can be very easy to assume the computer just knows what it's doing. For example, let's say I use an image‑generating AI like DALL‑E or Midjourney, and I give it a prompt that describes a fairly unique combination of objects and different qualities I want from this new image. I want a cinematic photograph of a golden skull on top of an old oak table in a library. And a few seconds later, I get several options of exactly what I was asking for. The AI has gone into its vast amount of training information, all the different examples of skulls and table and library. It knows what golden things look like. It knows about the qualities of a photograph as opposed to an oil painting or a pencil sketch. It knows things described as cinematic would have out‑of‑focus backgrounds. We get very impressive results. But if I give it a prompt that might even at first glance seem simpler, but requires a little deeper understanding, we can very quickly start to test the edges of this. So I'm going to use a prompt here and see if an image comes to mind as I say. I want a photo of a boy looking at an egg he's just dropped on a concrete floor, and here's the results I get. Now they do look really good. But any human artist would have thought, well, what happens when you drop an egg on a concrete floor. It's going to break. But the AI doesn't do that because these generative AIs do not have a fundamental knowledge of the world. They don't understand consequences. They don't understand true and false. They are amazing at matching patterns and making new combinations, but that doesn't mean they actually understand anything. Now if you're thinking yeah, yeah, yeah, but I don't really need to generate these kinds of image. Well know that this same issue also applies with text‑based generative AI like ChatGPT and Google Bard. These AIs all do their best to generate new results that are convincing and believable. It doesn't mean the results they generate are actually correct. There's even a term in generative AI called a hallucination where the AI basically makes up the answer. It might sound right. It's sometimes incredibly convincing, but it's not true. Here's one simple example. I was recently giving a demo of ChatGPT to a friend who works in marketing. So I asked ChatGPT, okay, write a blog post about the metaverse and talk about new hardware. And it did. In fact, it wrote a pretty good article about the metaverse, what's going on, and then it told me about the new DreamView XR‑2023 headset. It gave me specifics about its 8K resolution and its 220‑degree field of view. There's no such device. It doesn't exist. This is a hallucination. Now what it generated is very close to being right. An article about the metaverse that described new hardware would be very much like this text. It would be, let's say, 95, 98% like this. It would just talk about a headset that actually existed. You see, while generative AI can give you amazing results, we're still at the point where you should fact check anything you get from it, particularly if you're using them as any kind of technical reference or technical explanation. Generative AI is fantastic, but it does bring new issues. Not just those hallucinations, but the potential impact of misinformation, either intentional or unintentional, along with issues around data privacy, content ownership, and many larger issues about the impact this will have on many professions. So let's finish by talking specifically about some of those current and expected future problems with AI.

## Ethics and Responsible AI

There's a growing awareness and concern about the impact and the risks of AI, particularly as we're very quickly relying more and more on these systems. And some of the consequences are direct; some of them are indirect and unintentional, but still significant. An often unintentional side effect is algorithmic bias. An example is where machine learning was used to streamline recruitment and hiring at a large tech company to try and recognize and classify good candidates more quickly. And the model was trained on 10 years of actual resumes and real data about the qualities of previous successful candidates. But in a field like computing, which is historically skewed towards more male candidates, just on pure numbers alone, most successful candidates over the last 10 years had been men, and that became part of what the machine learning model would then look for. That's what we trained it to learn. What does a successful candidate look like? Well, according to the data, it looks like a man. So, does this new candidate match that description? No. Well, then, score them lower. So even when there was no intentional human prejudice involved, just in this case the nature of a male‑heavy profession, a machine learning model can end up reinforcing that inequality. And figuring out this is even happening can be very difficult because one of the very real issues with many algorithms in machine learning is that when you train a model, what you get from it after that is an answer. What you don't get is an explanation of that answer. This is sometimes called the black box of AI or the black box of machine learning. You train the model with existing data, you then feed new data into the model, and you get a result. Candidate A is rated 5 out of 5; candidate B is rated 2 out of 5\. Why? Well, it doesn't tell you why because the algorithm was designed to provide a result. It wasn't designed to provide an explanation. And if you have an unquestioning culture of, well, that's what the computer says, then you could end up reinforcing biases in the system, even building a stronger bias system without realizing it. In addition, there's the issue of accountability because it can be genuinely difficult to figure out who's responsible if an AI‑generated product gives a wrong decision. Now in some use cases, there's now a move towards what's called explainable AI or interpretable AI, which is where you would choose AI techniques that do not use that black box approach, and they provide a human‑readable explanation of results. In some governance and regulatory policies like the EU's GDPR, they're starting to include the idea of a right to explanation. So in a situation like a credit being refused or a job application being rejected by an automated system, the system must then provide an explanation and accountability of it. Now we also have techniques like deepfakes where AI is used for intentional deception and misinformation. A deepfake usually refers to a video, but it can also be just an image or an audio recording, but where AI is used to either replace a person or make it seem like they said something that they didn't. And deepfakes are typically made using a form of generative AI. They're using these deep neural networks to create extremely realistic results. And because of these, and there are plenty of other situations, there are now various initiatives around ethics in artificial intelligence and this idea of responsible AI. Now these days, most major tech organizations, just as they share their annual reports or their information about sustainability efforts, they also provide explicit information about their AI ethics and principles, often including at least information around how do we work with data privacy, how do we avoid bias, and what do we do with transparency and explainability. And standards organizations, like the IEEE, have their Global Initiative on Ethics of Autonomous and Intelligent Systems. There's groups like Partnership on AI with over 100 member organizations including Apple, Microsoft, Amazon, Google, Accenture, all trying to develop guidelines and best practices about safety and transparency in artificial intelligence. And this idea of ethical and safe behavior in AI is part of the stated mission of those organizations that are trying to develop artificial general intelligence, or AGI, because there is an understanding. It's incredibly important, and it's not just going to happen by itself. We've seen a lot of AI researchers, tech leaders, scientists all raising serious concerns about what's likely to happen if we're not thinking about how to create ethical and responsible AI well in advance. Or as Professor Stephen Hawking once said, success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last. Interesting times ahead. Hope you enjoyed the course. I'll see you next time.  
